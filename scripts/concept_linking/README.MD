Concept linking webapi
======================

This script runs a python Flask server to create a entity 
linking dataset. It requires a file containing a 
json array with qId, qText and a list of concepts.
We included one, concepts_moviesC-train.json, created
from moviesC-train dataset.

To start this script, run:

	python concept_linking_webapi.py concepts_moviesC-train.json output.json

The web itself is at localhost:5000/, there is a list of 
questions. When you click on one, you can choose the
correct concepts. The labels link to the corresponding
wikipedia entries for easy checking. If a concept is 
missing, you can search it in the wikipedia box to the
right. Then copy the label to the "unlisted" text boxes. 
The system will try to find the pageID in dbpedia, so it
might not find anything if you put in extra spaces and 
nonunicode characters.

Creating New Dataset
--------------------

To create your own dataset, you can use preprocess.py.
It requires the data from yodaQA questionDump and the 
original dataset.

To run it:

	python preprocess.py questionDump.json originalDataset.json concepts_Dataset.json

The result is appended to output.json. The output file 
is not a valid json, it requires postprocessing:

You can use the postprocessing script like this:

    python postprocess.py dataset_from_webapi.json train.json output.json

The script creates a valid json, checks for missing/duplicate
entries and sorts it. You can run the functions separately.

Measuring Entity Linking Performance
------------------------------------

The concept_linking_performance.py script takes the fixed 
QuestionDump json (generated by the entity linker we are benchmarking),
the gold standard tsv and the sorted dataset and measures the performance. 

    python concept_linking_performance.py questionDump.json correct_dataset.json GS_dump.tsv

It prints both missing and superfluous concepts for every question.
Also, it generates statistics at the end. Exact/partial matches,
precision, recall, F1. For each error case (exact match, superfluous
concepts, missing concepts, no concepts at all), answer MRR is calculated.

### v1.2 Entity Linking Baseline

For moviesC:

    python3 concept_linking_performance.py concepts_moviesC-train.json ../../moviesC/entity-linking.json moviesC-train-ovt-ua770e5f.tsv | less

We found 584 out of 721 (80.9%) correct concepts in total.
However, the precision is 584/3518 (16.6%), as we generated 
considerably more concepts. F1 score is 27.5%

We found the exact match for 252 questions (out of 542) and had
partial matches in 221 questions. 69 questions were linked to
wrong concepts or didn't have any concepts at all. Thus, the 
per-question precision is 57.9%, recall is at 81.7% and
F1 score is 27.5%.

	:: answer MRR per entity error type
	MRR for questions with exact match of concepts: 0.6215765688382541
	MRR for questions with superfluous concepts:    0.5116708115898956
	MRR for questions with missing concepts:        0.42734122723188556
	MRR for questions with no concepts at all:      0.0070153434508131605
	total MRR: 0.5214830605161728
